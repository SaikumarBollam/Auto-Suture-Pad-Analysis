{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (8.3.78)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.20.1-cp313-cp313-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: opencv-python in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: matplotlib in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: torch in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (0.21.0)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.6.0-cp313-cp313-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: packaging in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from onnxruntime) (24.2)\n",
      "Collecting protobuf (from onnxruntime)\n",
      "  Downloading protobuf-6.30.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: sympy in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\courses\\ml-based-suture-pad-analysis\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading onnxruntime-1.20.1-cp313-cp313-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/11.3 MB 3.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/11.3 MB 6.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 9.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.1/11.3 MB 14.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 15.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.2/11.3 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.3/11.3 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.3 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 15.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.9/11.3 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.3 MB 15.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.3/11.3 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.3 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.3 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.6/11.3 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 15.3 MB/s eta 0:00:00\n",
      "Using cached torchaudio-2.6.0-cp313-cp313-win_amd64.whl (2.4 MB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading protobuf-6.30.0-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/431.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 431.0/431.0 kB 18.9 MB/s eta 0:00:00\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.8/86.8 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.2/83.2 kB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: flatbuffers, pyreadline3, protobuf, humanfriendly, torchaudio, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-1.20.1 protobuf-6.30.0 pyreadline3-3.5.4 torchaudio-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics onnxruntime opencv-python numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save path\n",
    "yolov12m_model_url = r\"D:\\Courses\\ML-based-suture-pad-analysis\\ml-models\\models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Model saved at D:\\Courses\\ML-based-suture-pad-analysis\\ml-models\\models\\yolo12m_saved.pt\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load the best-trained model (Update the path once YOLOv12 is available)\n",
    "model_path = \"yolo12m.pt\"  # Update with YOLOv12 model when available\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "os.makedirs(yolov12m_model_url, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Save the model\n",
    "save_path = os.path.join(yolov12m_model_url, \"yolo12m_saved.pt\")\n",
    "model.save(save_path)\n",
    "\n",
    "print(f\"Model saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"incision_bent\",\n",
    "    1: \"incision_good\",\n",
    "    2: \"incision_perfect\",\n",
    "    3: \"knot_good\",\n",
    "    4: \"knot_loose\",\n",
    "    5: \"knot_perfect\",\n",
    "    6: \"knot_tight\",\n",
    "    7: \"suture_good\",\n",
    "    8: \"suture_loose\",\n",
    "    9: \"suture_perfect\",\n",
    "    10: \"suture_tight\",\n",
    "    11: \"tail_end\",\n",
    "    12: \"tail_top\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolov12_inference(image_path, conf_threshold=0.3, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run YOLOv12 on an image and return detections.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Perform detection\n",
    "    results = model.predict(source=image_path, conf=conf_threshold, iou=iou_threshold, device=device)\n",
    "\n",
    "    # Extract detections\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            conf = float(box.conf)\n",
    "            bbox = box.xyxy.cpu().numpy().tolist()[0]  # [x1, y1, x2, y2]\n",
    "            detections.append((class_id, conf, bbox))\n",
    "\n",
    "    return detections, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image_path, detections):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on the image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    for class_id, conf, bbox in detections:\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        label = f\"{class_names[class_id]}: {conf:.2f}\"\n",
    "        color = (0, 255, 0)  # Green for detected objects\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_detected_objects(image_path, detections, save_dir=\"cropped_sutures\"):\n",
    "    \"\"\"\n",
    "    Crop and save detected objects.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    for idx, (class_id, conf, bbox) in enumerate(detections):\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cropped_img = img[y1:y2, x1:x2]\n",
    "\n",
    "        if cropped_img.size == 0:\n",
    "            continue\n",
    "\n",
    "        cropped_filename = os.path.join(save_dir, f\"{class_names[class_id]}_{idx}.jpg\")\n",
    "        cv2.imwrite(cropped_filename, cropped_img)\n",
    "\n",
    "    print(f\"Cropped images saved in {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sample_suture.jpg\"  # Replace with your image\n",
    "detections, img = run_yolov12_inference(image_path)\n",
    "\n",
    "if detections:\n",
    "    visualize_detections(image_path, detections)\n",
    "    crop_detected_objects(image_path, detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolov12_to_onnx(model_path=\"yolov12m.pt\", output_onnx=\"yolov12.onnx\"):\n",
    "    \"\"\"\n",
    "    Convert YOLOv12 PyTorch model to ONNX.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    model.export(format=\"onnx\", dynamic=True, device=device)\n",
    "    print(f\"Model converted to {output_onnx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx>=1.12.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading onnx-1.17.0.tar.gz (12.2 MB)\n",
      "     ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.2 MB 385.2 kB/s eta 0:00:32\n",
      "     --------------------------------------- 0.1/12.2 MB 660.4 kB/s eta 0:00:19\n",
      "      --------------------------------------- 0.3/12.2 MB 2.0 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.3/12.2 MB 7.0 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.2 MB 14.1 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.4/12.2 MB 19.0 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.9/12.2 MB 24.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.0/12.2 MB 26.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 10.5/12.2 MB 43.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 11.4/12.2 MB 36.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.2/12.2 MB 38.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting onnxslim\n",
      "  Downloading onnxslim-0.1.48-py3-none-any.whl.metadata (4.6 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "ERROR: Could not find a version that satisfies the requirement pyproject.toml-based (from versions: none)\n",
      "ERROR: No matching distribution found for pyproject.toml-based\n",
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-cache-dir \"onnx>=1.12.0\" \"onnxslim\" pyproject.toml-based projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.78  Python-3.13.1 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv12m summary (fused): 169 layers, 20,166,592 parameters, 0 gradients, 67.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\Courses\\ML-based-suture-pad-analysis\\ml-models\\models\\yolo12m_saved.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (39.0 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim'] not found, attempting AutoUpdate...\n",
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"onnx>=1.12.0\" \"onnxslim\" ' returned non-zero exit status 1.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"onnx>=1.12.0\" \"onnxslim\" ' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache-dir \"onnx>=1.12.0\" \"onnxslim\" ' returned non-zero exit status 1.\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export failure  132.2s: No module named 'onnx'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run conversion\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mconvert_yolov12_to_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_onnx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov12.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mconvert_yolov12_to_onnx\u001b[1;34m(model_path, output_onnx)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mConvert YOLOv12 PyTorch model to ONNX.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(model_path)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel converted to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_onnx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Courses\\ML-based-suture-pad-analysis\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:742\u001b[0m, in \u001b[0;36mModel.export\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m custom \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    740\u001b[0m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[0;32m    741\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Courses\\ML-based-suture-pad-analysis\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:410\u001b[0m, in \u001b[0;36mExporter.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    408\u001b[0m     f[\u001b[38;5;241m1\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_engine(dla\u001b[38;5;241m=\u001b[39mdla)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m onnx:  \u001b[38;5;66;03m# ONNX\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m     f[\u001b[38;5;241m2\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xml:  \u001b[38;5;66;03m# OpenVINO\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     f[\u001b[38;5;241m3\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_openvino()\n",
      "File \u001b[1;32md:\\Courses\\ML-based-suture-pad-analysis\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:180\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    179\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export failure ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\Courses\\ML-based-suture-pad-analysis\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:175\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Profile() \u001b[38;5;28;01mas\u001b[39;00m dt:\n\u001b[1;32m--> 175\u001b[0m         f, model \u001b[38;5;241m=\u001b[39m \u001b[43minner_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export success ✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size(f)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, model\n",
      "File \u001b[1;32md:\\Courses\\ML-based-suture-pad-analysis\\.venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:509\u001b[0m, in \u001b[0;36mExporter.export_onnx\u001b[1;34m(self, prefix)\u001b[0m\n\u001b[0;32m    507\u001b[0m     requirements \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxslim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxruntime\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    508\u001b[0m check_requirements(requirements)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnx\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    511\u001b[0m opset_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mopset \u001b[38;5;129;01mor\u001b[39;00m get_latest_opset()\n\u001b[0;32m    512\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starting export with onnx \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'onnx'"
     ]
    }
   ],
   "source": [
    "# Run conversion\n",
    "convert_yolov12_to_onnx(model_path=save_path, output_onnx=\"yolov12.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model = \"yolov12.onnx\"\n",
    "session = ort.InferenceSession(onnx_model, providers=[\"CUDAExecutionProvider\"] if torch.cuda.is_available() else [\"CPUExecutionProvider\"])\n",
    "\n",
    "def run_onnx_inference(image_path):\n",
    "    \"\"\"\n",
    "    Run inference using ONNX Runtime for faster processing.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_resized = cv2.resize(img, (640, 640))\n",
    "    img_resized = np.expand_dims(img_resized.transpose(2, 0, 1), axis=0).astype(np.float32)\n",
    "\n",
    "    inputs = {session.get_inputs()[0].name: img_resized}\n",
    "    outputs = session.run(None, inputs)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Run ONNX inference\n",
    "onnx_results = run_onnx_inference(image_path)\n",
    "print(\"ONNX Inference Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
